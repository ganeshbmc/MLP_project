{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fd470976",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:25.034876Z",
     "iopub.status.busy": "2023-06-04T09:36:25.034436Z",
     "iopub.status.idle": "2023-06-04T09:36:25.049695Z",
     "shell.execute_reply": "2023-06-04T09:36:25.048326Z"
    },
    "papermill": {
     "duration": 0.028681,
     "end_time": "2023-06-04T09:36:25.052021",
     "exception": false,
     "start_time": "2023-06-04T09:36:25.023340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47faf272",
   "metadata": {},
   "source": [
    "## Import remaining libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ce58f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fc65718",
   "metadata": {},
   "source": [
    "## Load datasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "386c4a8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:25.071147Z",
     "iopub.status.busy": "2023-06-04T09:36:25.070780Z",
     "iopub.status.idle": "2023-06-04T09:36:26.109926Z",
     "shell.execute_reply": "2023-06-04T09:36:26.108634Z"
    },
    "papermill": {
     "duration": 1.05149,
     "end_time": "2023-06-04T09:36:26.112273",
     "exception": false,
     "start_time": "2023-06-04T09:36:25.060783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162758, 5)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = pd.read_csv(\"data/train.csv\")\n",
    "traindf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7618826f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:26.131856Z",
     "iopub.status.busy": "2023-06-04T09:36:26.131442Z",
     "iopub.status.idle": "2023-06-04T09:36:26.466084Z",
     "shell.execute_reply": "2023-06-04T09:36:26.464836Z"
    },
    "papermill": {
     "duration": 0.347849,
     "end_time": "2023-06-04T09:36:26.468982",
     "exception": false,
     "start_time": "2023-06-04T09:36:26.121133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55315, 4)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = pd.read_csv(\"data/test.csv\")\n",
    "testdf.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de354e4a",
   "metadata": {},
   "source": [
    "## Getting data ready for training and evaluating models  \n",
    "\n",
    "`Handle missing data. Current strategy is to fill in some custom text.`  \n",
    "`Separate features and labels from traindf`  \n",
    "`Split training data into training and testing parts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7b712825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162758, 5)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ec11cada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 162758 entries, 0 to 162757\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   movieid             162758 non-null  object\n",
      " 1   reviewerName        162758 non-null  object\n",
      " 2   isFrequentReviewer  162758 non-null  bool  \n",
      " 3   reviewText          156311 non-null  object\n",
      " 4   sentiment           162758 non-null  object\n",
      "dtypes: bool(1), object(4)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "traindf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a804ad27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieid                  0\n",
       "reviewerName             0\n",
       "isFrequentReviewer       0\n",
       "reviewText            6447\n",
       "sentiment                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dab1d8da",
   "metadata": {},
   "source": [
    "### Fill empty reviewText with custom text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9b45c97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieid               0\n",
       "reviewerName          0\n",
       "isFrequentReviewer    0\n",
       "reviewText            0\n",
       "sentiment             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf[\"reviewText\"].fillna(\"empty\", inplace=True)\n",
    "traindf.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ba3d9db",
   "metadata": {},
   "source": [
    "### Separate features and labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e1f3ee64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((162758,), (162758,))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = traindf[\"reviewText\"]\n",
    "train_labels = traindf.iloc[:, -1]\n",
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9427b57a",
   "metadata": {},
   "source": [
    "### Split traindf into training and testing parts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "46324c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((122068,), (40690,), (122068,), (40690,))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=0.25, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcbe13f6",
   "metadata": {},
   "source": [
    "### Needs further exploration  \n",
    "\n",
    "`stratify based on reviewer type?`  \n",
    "`play with test size`  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "221b00ef",
   "metadata": {},
   "source": [
    "## Model evaluation plan and code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5172e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1fe404fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    # print(f1_score(y_test, y_pred))\n",
    "    ConfusionMatrixDisplay(y_test, y_pred)\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7d6d772",
   "metadata": {},
   "source": [
    "# Data preprocessing  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "325829c9",
   "metadata": {
    "papermill": {
     "duration": 0.00978,
     "end_time": "2023-06-04T09:36:28.312103",
     "exception": false,
     "start_time": "2023-06-04T09:36:28.302323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define stop words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "58a0bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d8fb9fc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:28.334924Z",
     "iopub.status.busy": "2023-06-04T09:36:28.334510Z",
     "iopub.status.idle": "2023-06-04T09:36:28.408334Z",
     "shell.execute_reply": "2023-06-04T09:36:28.407527Z"
    },
    "papermill": {
     "duration": 0.088641,
     "end_time": "2023-06-04T09:36:28.410746",
     "exception": false,
     "start_time": "2023-06-04T09:36:28.322105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1160)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
    "type(stop_words), len(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8d2bcae",
   "metadata": {},
   "source": [
    "## Feature extraction  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cbd54a9",
   "metadata": {},
   "source": [
    "### Understand CountVectorizer and TfidfVectorizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "25f00458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = X_train.copy()\n",
    "# text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "893c79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "# vectorized_text = vectorizer.fit_transform(text[\"reviewText\"])\n",
    "# vectorized_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5877de92",
   "metadata": {},
   "source": [
    "`Without stop words`  \n",
    "<122068x58624 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 2312222 stored elements in Compressed Sparse Row format>\n",
    "\n",
    "`With stop words`  \n",
    "<122068x57784 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 1188395 stored elements in Compressed Sparse Row format>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0dbcac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer2 = CountVectorizer(stop_words=stop_words)\n",
    "# vectorized_text2 = vectorizer2.fit_transform(text[\"reviewText\"])\n",
    "# vectorized_text2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a35692b",
   "metadata": {},
   "source": [
    "`Without stop words`  \n",
    "<122068x58624 sparse matrix of type '<class 'numpy.int64'>'\n",
    "\twith 2312222 stored elements in Compressed Sparse Row format>\n",
    "\n",
    "`With stop words`  \n",
    "<122068x57784 sparse matrix of type '<class 'numpy.int64'>'\n",
    "\twith 1188395 stored elements in Compressed Sparse Row format>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bef671ec",
   "metadata": {},
   "source": [
    "### Conclusion   \n",
    "`TfidfVectorizer is equivalent to CountVectorizer followed by TfidfTransformer.`  \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b77273f",
   "metadata": {},
   "source": [
    "## Model building workflow  \n",
    "\n",
    "`Make pipeline for preprocesing and model training and predictions`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6b6b8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "87480f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_n_train(X_train, y_train, preprocessor, model):\n",
    "    pipe = Pipeline(steps=[\n",
    "                            (\"preprocessor\", preprocessor),\n",
    "                            (\"model\", model)\n",
    "                        ])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Given model has been trained. Use predict method to get predictions array.\")\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict_n_evaluate(pipeline, X_test, y_test):\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(f\"y_pred shape: {y_pred.shape}\")\n",
    "    print(f\"Summary of predictions: {np.unique(y_pred, return_counts=True)}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return y_pred\n",
    "\n",
    "# def evaluate(y_test, y_pred):\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "#     # print(f1_score(y_test, y_pred))\n",
    "#     ConfusionMatrixDisplay(y_test, y_pred)\n",
    "#     return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38cd7096",
   "metadata": {},
   "source": [
    "## Preprocessor(s)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "117d2730",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "tvec_sw = TfidfVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "280cb6d1",
   "metadata": {
    "papermill": {
     "duration": 0.014062,
     "end_time": "2023-06-04T09:36:28.437109",
     "exception": false,
     "start_time": "2023-06-04T09:36:28.423047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Logistic Regression with TfidVectorizer for preprocessing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4e819d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:26.626132Z",
     "iopub.status.busy": "2023-06-04T09:36:26.625005Z",
     "iopub.status.idle": "2023-06-04T09:36:28.289756Z",
     "shell.execute_reply": "2023-06-04T09:36:28.288573Z"
    },
    "papermill": {
     "duration": 1.678618,
     "end_time": "2023-06-04T09:36:28.292575",
     "exception": false,
     "start_time": "2023-06-04T09:36:26.613957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3af63acf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:28.461996Z",
     "iopub.status.busy": "2023-06-04T09:36:28.461543Z",
     "iopub.status.idle": "2023-06-04T09:36:28.475204Z",
     "shell.execute_reply": "2023-06-04T09:36:28.474077Z"
    },
    "papermill": {
     "duration": 0.027907,
     "end_time": "2023-06-04T09:36:28.477774",
     "exception": false,
     "start_time": "2023-06-04T09:36:28.449867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cab0bd13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:28.499904Z",
     "iopub.status.busy": "2023-06-04T09:36:28.499114Z",
     "iopub.status.idle": "2023-06-04T09:36:28.517143Z",
     "shell.execute_reply": "2023-06-04T09:36:28.516003Z"
    },
    "papermill": {
     "duration": 0.032035,
     "end_time": "2023-06-04T09:36:28.519650",
     "exception": false,
     "start_time": "2023-06-04T09:36:28.487615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TfidfVectorizer(), CountVectorizer())"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "cvec = CountVectorizer()        # Note that TfidfVectorizer and CountVectorizer+TfidsTransformer do the same function\n",
    "tvec, cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b11e56fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:28.542840Z",
     "iopub.status.busy": "2023-06-04T09:36:28.541997Z",
     "iopub.status.idle": "2023-06-04T09:36:28.600559Z",
     "shell.execute_reply": "2023-06-04T09:36:28.599462Z"
    },
    "papermill": {
     "duration": 0.073306,
     "end_time": "2023-06-04T09:36:28.603280",
     "exception": false,
     "start_time": "2023-06-04T09:36:28.529974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given model has been trained. Use predict method to get predictions array.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor', TfidfVectorizer()),\n",
       "                ('model', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_pipe = preprocess_n_train(X_train, y_train, tvec, logreg)\n",
    "logreg_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d987cd2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:55.516665Z",
     "iopub.status.busy": "2023-06-04T09:36:55.515673Z",
     "iopub.status.idle": "2023-06-04T09:36:57.126863Z",
     "shell.execute_reply": "2023-06-04T09:36:57.126080Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1.625654,
     "end_time": "2023-06-04T09:36:57.129043",
     "exception": false,
     "start_time": "2023-06-04T09:36:55.503389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape: (40690,)\n",
      "Summary of predictions: (array(['NEGATIVE', 'POSITIVE'], dtype=object), array([10012, 30678], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.76      0.57      0.65     13430\n",
      "    POSITIVE       0.81      0.91      0.86     27260\n",
      "\n",
      "    accuracy                           0.80     40690\n",
      "   macro avg       0.79      0.74      0.75     40690\n",
      "weighted avg       0.79      0.80      0.79     40690\n",
      "\n",
      "[[ 7619  5811]\n",
      " [ 2393 24867]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE', 'NEGATIVE', 'NEGATIVE', ..., 'POSITIVE', 'POSITIVE',\n",
       "       'POSITIVE'], dtype=object)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_logreg = predict_n_evaluate(logreg_pipe, X_test, y_test)\n",
    "y_pred_logreg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84dd83b6",
   "metadata": {},
   "source": [
    "### Logistic Regression with stop words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8fb584d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_sw = TfidfVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "dfd9c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\All users\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['articl', 'mon'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given model has been trained. Use predict method to get predictions array.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 TfidfVectorizer(stop_words=['0o', '0s', '3a', '3b', '3d', '6b',\n",
       "                                             '6o', 'a', 'a1', 'a2', 'a3', 'a4',\n",
       "                                             'ab', 'able', 'about', 'above',\n",
       "                                             'abst', 'ac', 'accordance',\n",
       "                                             'according', 'accordingly',\n",
       "                                             'across', 'act', 'actually', 'ad',\n",
       "                                             'added', 'adj', 'ae', 'af',\n",
       "                                             'affected', ...])),\n",
       "                ('model', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_sw_pipe = preprocess_n_train(X_train, y_train, tvec_sw, logreg)\n",
    "logreg_sw_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9447109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape: (40690,)\n",
      "Summary of predictions: (array(['NEGATIVE', 'POSITIVE'], dtype=object), array([ 9168, 31522], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.76      0.52      0.62     13430\n",
      "    POSITIVE       0.79      0.92      0.85     27260\n",
      "\n",
      "    accuracy                           0.79     40690\n",
      "   macro avg       0.78      0.72      0.73     40690\n",
      "weighted avg       0.78      0.79      0.77     40690\n",
      "\n",
      "[[ 6965  6465]\n",
      " [ 2203 25057]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE', 'NEGATIVE', 'POSITIVE', ..., 'POSITIVE', 'POSITIVE',\n",
       "       'POSITIVE'], dtype=object)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_logreg_sw = predict_n_evaluate(logreg_sw_pipe, X_test, y_test)\n",
    "y_pred_logreg_sw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7673ff0",
   "metadata": {},
   "source": [
    "## SVM model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "675a3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c0c15755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5d1e0b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given model has been trained. Use predict method to get predictions array.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor', TfidfVectorizer()), ('model', LinearSVC())])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pipe = preprocess_n_train(X_train, y_train, tvec, svm)\n",
    "svm_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "18d5547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape: (40690,)\n",
      "Summary of predictions: (array(['NEGATIVE', 'POSITIVE'], dtype=object), array([11525, 29165], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.72      0.62      0.66     13430\n",
      "    POSITIVE       0.82      0.88      0.85     27260\n",
      "\n",
      "    accuracy                           0.79     40690\n",
      "   macro avg       0.77      0.75      0.76     40690\n",
      "weighted avg       0.79      0.79      0.79     40690\n",
      "\n",
      "[[ 8284  5146]\n",
      " [ 3241 24019]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE', 'NEGATIVE', 'POSITIVE', ..., 'POSITIVE', 'POSITIVE',\n",
       "       'POSITIVE'], dtype=object)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_svm = predict_n_evaluate(svm_pipe, X_test, y_test)\n",
    "y_pred_svm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bbe7a9a",
   "metadata": {},
   "source": [
    "### SVM model with stop words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "22b3a5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given model has been trained. Use predict method to get predictions array.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 TfidfVectorizer(stop_words=['0o', '0s', '3a', '3b', '3d', '6b',\n",
       "                                             '6o', 'a', 'a1', 'a2', 'a3', 'a4',\n",
       "                                             'ab', 'able', 'about', 'above',\n",
       "                                             'abst', 'ac', 'accordance',\n",
       "                                             'according', 'accordingly',\n",
       "                                             'across', 'act', 'actually', 'ad',\n",
       "                                             'added', 'adj', 'ae', 'af',\n",
       "                                             'affected', ...])),\n",
       "                ('model', LinearSVC())])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_sw_pipe = preprocess_n_train(X_train, y_train, tvec_sw, svm)\n",
    "svm_sw_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e08cc61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape: (40690,)\n",
      "Summary of predictions: (array(['NEGATIVE', 'POSITIVE'], dtype=object), array([11111, 29579], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.70      0.58      0.63     13430\n",
      "    POSITIVE       0.81      0.88      0.84     27260\n",
      "\n",
      "    accuracy                           0.78     40690\n",
      "   macro avg       0.75      0.73      0.74     40690\n",
      "weighted avg       0.77      0.78      0.77     40690\n",
      "\n",
      "[[ 7760  5670]\n",
      " [ 3351 23909]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE', 'NEGATIVE', 'POSITIVE', ..., 'POSITIVE', 'POSITIVE',\n",
       "       'POSITIVE'], dtype=object)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_svm_sw = predict_n_evaluate(svm_sw_pipe, X_test, y_test)\n",
    "y_pred_svm_sw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4b3cb9",
   "metadata": {},
   "source": [
    "## Naive Bayes model  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a6092f9",
   "metadata": {},
   "source": [
    "### MultinomialNB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d8404b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB, ComplementNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a24f5f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "89a375b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given model has been trained. Use predict method to get predictions array.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor', TfidfVectorizer()),\n",
       "                ('model', MultinomialNB())])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_pipe = preprocess_n_train(X_train, y_train, tvec, mnb)\n",
    "mnb_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e755cefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape: (40690,)\n",
      "Summary of predictions: (array(['NEGATIVE', 'POSITIVE'], dtype='<U8'), array([ 3936, 36754], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.88      0.26      0.40     13430\n",
      "    POSITIVE       0.73      0.98      0.84     27260\n",
      "\n",
      "    accuracy                           0.74     40690\n",
      "   macro avg       0.80      0.62      0.62     40690\n",
      "weighted avg       0.78      0.74      0.69     40690\n",
      "\n",
      "[[ 3467  9963]\n",
      " [  469 26791]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE', 'NEGATIVE', 'POSITIVE', ..., 'POSITIVE', 'POSITIVE',\n",
       "       'POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_mnb = predict_n_evaluate(mnb_pipe, X_test, y_test)\n",
    "y_pred_mnb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c21594b9",
   "metadata": {},
   "source": [
    "### GaussianNB  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82c4922d",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4a7a637a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c6e864f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GANESH~1\\AppData\\Local\\Temp/ipykernel_17584/3805403189.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgnb_pipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_n_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgnb_pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GANESH~1\\AppData\\Local\\Temp/ipykernel_17584/1383048372.py\u001b[0m in \u001b[0;36mpreprocess_n_train\u001b[1;34m(X_train, y_train, preprocessor, model)\u001b[0m\n\u001b[0;32m      5\u001b[0m                         ])\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Given model has been trained. Use predict method to get predictions array.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\All users\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\All users\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m--> 207\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n",
      "\u001b[1;32mc:\\Users\\All users\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\All users\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\All users\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\All users\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\All users\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    651\u001b[0m                                       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m                                       \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\All users\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[0;32m    418\u001b[0m                         \u001b[1;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "gnb_pipe = preprocess_n_train(X_train, y_train, tvec, gnb)\n",
    "gnb_pipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4f59597",
   "metadata": {},
   "source": [
    "## Submitting predictions to Kaggle competition  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a9de2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:57.275673Z",
     "iopub.status.busy": "2023-06-04T09:36:57.275229Z",
     "iopub.status.idle": "2023-06-04T09:36:57.281558Z",
     "shell.execute_reply": "2023-06-04T09:36:57.280375Z"
    },
    "papermill": {
     "duration": 0.021619,
     "end_time": "2023-06-04T09:36:57.284021",
     "exception": false,
     "start_time": "2023-06-04T09:36:57.262402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def submit(y_pred):\n",
    "#     pred_df = pd.DataFrame(y_pred)\n",
    "#     pred_df.columns = [\"sentiment\"]\n",
    "#     pred_df.index.name = \"id\"\n",
    "#     pred_df.to_csv(\"submission.csv\")\n",
    "#     return \"Successfully created the submission file!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeec35f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T09:36:57.308784Z",
     "iopub.status.busy": "2023-06-04T09:36:57.308373Z",
     "iopub.status.idle": "2023-06-04T09:36:57.485648Z",
     "shell.execute_reply": "2023-06-04T09:36:57.484607Z"
    },
    "papermill": {
     "duration": 0.192489,
     "end_time": "2023-06-04T09:36:57.488255",
     "exception": false,
     "start_time": "2023-06-04T09:36:57.295766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 46.244844,
   "end_time": "2023-06-04T09:36:58.791799",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-04T09:36:12.546955",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
