{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from itertools import compress\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB, ComplementNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# import lightgbm as ltb\n",
    "\n",
    "import scipy.stats as stats\n",
    "print(\"Imports done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'vscode'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for loading files  \n",
    "\n",
    "def load_csv(filename: str):\n",
    "    if platform == \"vscode\":\n",
    "        df = pd.read_csv(f\"data/{filename}.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv(f\"/kaggle/input/sentiment-prediction-on-movie-reviews/{filename}.csv\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(df: pd.DataFrame):\n",
    "    print(f\"Shape of the dataframe: {df.shape}\")\n",
    "    print()\n",
    "    print(f\"Columns in the dataframe:\\n{df.columns}\")\n",
    "    print()\n",
    "    print(f\"{df.info()}\")\n",
    "    print()\n",
    "    # print(f\"Summary: {df.describe()}\")\n",
    "    print(f\"Missing values:\\n{df.isna().sum()}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_fl(name):\n",
    "    l = name.split()\n",
    "    n = ' '.join((l[0], l[-1]))\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(df: pd.DataFrame, moviesdf: pd.DataFrame, row_thresh_null=None):\n",
    "    '''\n",
    "    This function merges the given dataframes. Note that the first df must be \"train\" or \"test\" and\n",
    "    the second df should be \"movies\".\n",
    "    Note: Sentiment column is present only in \"train.csv\" file and not \"test.csv\" file.\n",
    "    '''\n",
    "    \n",
    "    # Drop duplicates from moviesdf\n",
    "#     movies_unique = moviesdf.drop_duplicates(subset=[\"movieid\"])\n",
    "\n",
    "    # Drop duplicates using groupby - clubs similar rows and fills in missing values better\n",
    "    movies_unique = moviesdf.fillna(value=np.nan).groupby(\"movieid\").first().reset_index()\n",
    "    \n",
    "\n",
    "    # Handle missing values in movies.csv better than just dropping duplicates?\n",
    "#     movies_unique = moviesdf.copy()\n",
    "#     movies_unique[[\"audienceScore\", \"runtimeMinutes\"]] = movies_unique[[\"audienceScore\", \"runtimeMinutes\"]].interpolate(method='linear', axis=0)\n",
    "#     movies_unique = movies_unique.fillna(value=np.nan).groupby(\"movieid\").first().fillna(method='ffill').reset_index()\n",
    "\n",
    "    \n",
    "\n",
    "    # Merge df and movies_unique\n",
    "    df_merged = pd.merge(df, movies_unique, on=\"movieid\", how='left')\n",
    "    \n",
    "    # Rename \"isTopCritic\" column, if it exists, to \"isFrequentReviewer\"\n",
    "    df_merged.rename(columns={\"isTopCritic\": \"isFrequentReviewer\"}, inplace=True)\n",
    "    \n",
    "    # Drop columns\n",
    "#     df_merged = df_merged.drop(columns=[\"title\", \"ratingContents\", \"releaseDateTheaters\", \"releaseDateStreaming\", \"distributor\", \"soundType\"])\n",
    "#     df_merged = df_merged.drop(columns=[\"title\", \"soundType\"])\n",
    "    \n",
    "    # Drop rows (OPTIONAL: Uses kwarg row_thresh_null)\n",
    "    if row_thresh_null != None:\n",
    "        df_merged.dropna(axis=0, thresh=(df_merged.shape[1] - row_thresh_null), inplace=True)\n",
    "        \n",
    "\n",
    "    # Create new columns based on reviewText\n",
    "    final = df_merged.copy()\n",
    "    final[\"reviewYN\"] = np.where(final[\"reviewText\"].isnull(), 1, 0)    # Feature engineering - adding a new column\n",
    "    final[\"reviewWC\"] = final.apply(lambda x: len(str(x[\"reviewText\"]).split()), axis=1)    # Feature engineering - adding second new column\n",
    "    \n",
    "    # Clean text (replace numbers with empty string) and fill missing values in \"reviewText\" with empty string\n",
    "    final[\"reviewText\"] = final[\"reviewText\"].str.replace('\\d+', '', regex=True)\n",
    "    final[\"reviewText\"] = final[\"reviewText\"].fillna(\"neutral\")\n",
    "    \n",
    "    # Fill missing values in \"rating\", \"genre\", original columns with the word \"unknown\"\n",
    "    final[\"rating\"] = final[\"rating\"].fillna(\"unknown\")\n",
    "    final[\"originalLanguage\"] = final[\"originalLanguage\"].fillna(\"unknown\")\n",
    "    final[\"genre\"] = final[\"genre\"].fillna(\"unknown\")\n",
    "    final[\"genre\"] = final[\"genre\"].apply(lambda x: re.sub(r\"-\", \"\", x))\n",
    "    final[\"genreSorted\"] = final[\"genre\"].apply(lambda x: (\",\").join(sorted(x.split(\", \"))))\n",
    "#     final[\"genre\"] = final[\"genre\"].replace(to_replace={\"&\": \"\"})\n",
    "\n",
    "    # Impute missing values for \"audienceScore\" and \"runtimeMinutes\" columns\n",
    "    final[\"audienceScore\"] = final[\"audienceScore\"].fillna(final[\"audienceScore\"].mean())\n",
    "    final[\"runtimeMinutes\"] = final[\"runtimeMinutes\"].fillna(final[\"runtimeMinutes\"].median())\n",
    "    \n",
    "    # Preprocess and impute missing values in \"boxOffice\" column\n",
    "    final[\"boxOffice\"] = final[\"boxOffice\"].str[1:]\n",
    "    final[\"boxOffice\"] = final[\"boxOffice\"].replace(to_replace={\"M\": \"*1000000\", \"K\": \"*1000\"}, regex=True)\n",
    "    final[\"boxOffice\"] = final[\"boxOffice\"].loc[final[\"boxOffice\"].notnull()].apply(lambda x: eval(str(x)))\n",
    "    final[\"boxOffice\"] = final[\"boxOffice\"].fillna(final[\"boxOffice\"].median())\n",
    "    # (Optional) Replace outliers in boxOffice with median\n",
    "#     median = final[\"boxOffice\"].describe()['50%']\n",
    "#     iqr = final[\"boxOffice\"].describe()['75%'] - final[\"boxOffice\"].describe()['25%']\n",
    "#     ll = median - (1.5*iqr)\n",
    "#     ul = median + (1.5*iqr)\n",
    "#     final.loc[final[\"boxOffice\"] > ul, \"boxOffice\"] = median\n",
    "    \n",
    "    # Clean language names\n",
    "    final[\"originalLanguage\"].replace({\"English (United Kingdom)\": \"English\", \n",
    "                                            \"English (Australia)\" : \"English\",\n",
    "                                            \"French (France)\": \"French\", \n",
    "                                            \"French (Canada)\": \"French\",\n",
    "                                            \"Portuguese (Brazil)\": \"Portuguese\",\n",
    "                                            \"Spanish (Spain)\": \"Spanish\"},                                         \n",
    "                                            inplace=True)\n",
    "    \n",
    "    # Clean reviewerName column\n",
    "    pre_post_fixes = {\"Mr. \": \"\", \"Mrs. \": \"\", \"Ms. \": \"\", \"Dr. \": \"\", \n",
    "                      \" MD\": \"\", \" DDS\": \"\", \" DVM\": \"\", \" Jr.\": \"\", \" PhD\": \"\", \" II\": \"\", \" IV\": \"\"}\n",
    "    final[\"reviewerName\"] = final[\"reviewerName\"].replace(pre_post_fixes, regex=True)\n",
    "    final[\"reviewerName\"] = final[\"reviewerName\"].apply(name_fl)\n",
    "    \n",
    "    # Handle 'ratingContents' column\n",
    "    final[\"ratingContents\"] = final[\"ratingContents\"].fillna(\"neutral\")\n",
    "    final[\"rcSorted\"] = final[\"ratingContents\"].apply(lambda x: (\",\").join(sorted(x.strip(\"][\").split(\", \"))))\n",
    "    final[\"rcSorted\"] = final[\"rcSorted\"].apply(lambda x: re.sub(r\"'\", \"\", x))\n",
    "    final[\"rcSorted\"] = final[\"rcSorted\"].apply(lambda x: re.sub(r\"[/\\s]\", \"_\", x))  \n",
    "    \n",
    "    # Handle 'ratingContents' column\n",
    "    final[\"distributor\"] = final[\"distributor\"].fillna(\"unknown\")\n",
    "    \n",
    "    # Work with 'releaseDateTheaters', releaseDateStreaming column\n",
    "    final[[\"releaseDateTheaters\", \"releaseDateStreaming\"]] = final[[\"releaseDateTheaters\", \"releaseDateStreaming\"]].astype('datetime64[ns]')\n",
    "\n",
    "    final[\"releaseDate\"] = final[[\"releaseDateTheaters\", \"releaseDateStreaming\"]].min(axis=1, skipna=False)\n",
    "    final[\"releaseDate\"] = final[\"releaseDate\"].fillna(final[\"releaseDate\"].median())\n",
    "\n",
    "\n",
    "    final[\"releaseYear\"] = final[\"releaseDate\"].dt.year\n",
    "    final[\"releaseMonth\"] = final[\"releaseDate\"].dt.month\n",
    "    \n",
    "    # Compute \"releaseDiff\" column and fill missing values in \"releaseDiff\" and (optional) replace outliers\n",
    "    final[\"releaseDiff\"] = (final[\"releaseDateStreaming\"] - final[\"releaseDateTheaters\"]) / np.timedelta64(1, 'D')\n",
    "    final[\"releaseDiff\"] = final[\"releaseDiff\"].apply(lambda x: abs(x))\n",
    "    final[\"releaseDiff\"] = final[\"releaseDiff\"].fillna(value=0)\n",
    "#     final[\"releaseDiff\"] = final[\"releaseDiff\"].fillna(final[\"releaseDiff\"].median())\n",
    "    # median = final[\"releaseDiff\"].describe()['50%']\n",
    "    # iqr = final[\"releaseDiff\"].describe()['75%'] - final[\"releaseDiff\"].describe()['25%']\n",
    "    # ll = median - (1.5*iqr)\n",
    "    # ul = median + (1.5*iqr)\n",
    "    # final.loc[final[\"releaseDiff\"] > ul, \"releaseDiff\"] = median\n",
    "    # final.loc[final[\"releaseDiff\"] < ll, \"releaseDiff\"] = median\n",
    "    \n",
    "    # Create new feature columns\n",
    "    \n",
    "    # Convert audienceScore to categories  \n",
    "    num_bins_as = 20\n",
    "    final[\"audScoreBins\"] = pd.cut(final['audienceScore'], bins=num_bins_as, labels=False)\n",
    "    \n",
    "    # Convert runtimeMinutes to categories  \n",
    "#     num_bins_rt = 20\n",
    "    final[\"runtimeBins\"] = pd.cut(final['runtimeMinutes'], bins=[0,75,120,180,565], labels=[4,3,2,1])\n",
    "    \n",
    "    # Convert boxOffice to categories  \n",
    "    num_bins_bo = 5\n",
    "    final[\"boxOfficeBins\"] = pd.cut(final['boxOffice'], bins=num_bins_bo, labels=False)\n",
    "    \n",
    "    # Convert releaseDiff to categories  \n",
    "    num_bins_rd = 5\n",
    "    final[\"releaseDiffBins\"] = pd.cut(final['releaseDiff'], bins=[-1, 180, 360, 1000, 40000], labels=[0, 1, 2, 3])\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_predict(features, labels, pipeline, test_size=0.25, random_state=42):\n",
    "    # cols = features.columns\n",
    "    if len(features.shape) == 1:\n",
    "        features = features.to_numpy().reshape(-1, 1)  # reshape to 2D array\n",
    "    features = pd.DataFrame(features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=random_state)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataframe: (162758, 30)\n",
      "\n",
      "Columns in the dataframe:\n",
      "Index(['movieid', 'reviewerName', 'isFrequentReviewer', 'reviewText',\n",
      "       'sentiment', 'title', 'audienceScore', 'rating', 'ratingContents',\n",
      "       'releaseDateTheaters', 'releaseDateStreaming', 'runtimeMinutes',\n",
      "       'genre', 'originalLanguage', 'director', 'boxOffice', 'distributor',\n",
      "       'soundType', 'reviewYN', 'reviewWC', 'genreSorted', 'rcSorted',\n",
      "       'releaseDate', 'releaseYear', 'releaseMonth', 'releaseDiff',\n",
      "       'audScoreBins', 'runtimeBins', 'boxOfficeBins', 'releaseDiffBins'],\n",
      "      dtype='object')\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 162758 entries, 0 to 162757\n",
      "Data columns (total 30 columns):\n",
      " #   Column                Non-Null Count   Dtype         \n",
      "---  ------                --------------   -----         \n",
      " 0   movieid               162758 non-null  object        \n",
      " 1   reviewerName          162758 non-null  object        \n",
      " 2   isFrequentReviewer    162758 non-null  bool          \n",
      " 3   reviewText            162758 non-null  object        \n",
      " 4   sentiment             162758 non-null  object        \n",
      " 5   title                 162758 non-null  object        \n",
      " 6   audienceScore         162758 non-null  float64       \n",
      " 7   rating                162758 non-null  object        \n",
      " 8   ratingContents        162758 non-null  object        \n",
      " 9   releaseDateTheaters   146403 non-null  datetime64[ns]\n",
      " 10  releaseDateStreaming  157353 non-null  datetime64[ns]\n",
      " 11  runtimeMinutes        162758 non-null  float64       \n",
      " 12  genre                 162758 non-null  object        \n",
      " 13  originalLanguage      162758 non-null  object        \n",
      " 14  director              162758 non-null  object        \n",
      " 15  boxOffice             162758 non-null  float64       \n",
      " 16  distributor           162758 non-null  object        \n",
      " 17  soundType             93466 non-null   object        \n",
      " 18  reviewYN              162758 non-null  int32         \n",
      " 19  reviewWC              162758 non-null  int64         \n",
      " 20  genreSorted           162758 non-null  object        \n",
      " 21  rcSorted              162758 non-null  object        \n",
      " 22  releaseDate           162758 non-null  datetime64[ns]\n",
      " 23  releaseYear           162758 non-null  int64         \n",
      " 24  releaseMonth          162758 non-null  int64         \n",
      " 25  releaseDiff           162758 non-null  float64       \n",
      " 26  audScoreBins          162758 non-null  int64         \n",
      " 27  runtimeBins           162758 non-null  category      \n",
      " 28  boxOfficeBins         162758 non-null  int64         \n",
      " 29  releaseDiffBins       162758 non-null  category      \n",
      "dtypes: bool(1), category(2), datetime64[ns](3), float64(4), int32(1), int64(5), object(14)\n",
      "memory usage: 34.6+ MB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      "movieid                     0\n",
      "reviewerName                0\n",
      "isFrequentReviewer          0\n",
      "reviewText                  0\n",
      "sentiment                   0\n",
      "title                       0\n",
      "audienceScore               0\n",
      "rating                      0\n",
      "ratingContents              0\n",
      "releaseDateTheaters     16355\n",
      "releaseDateStreaming     5405\n",
      "runtimeMinutes              0\n",
      "genre                       0\n",
      "originalLanguage            0\n",
      "director                    0\n",
      "boxOffice                   0\n",
      "distributor                 0\n",
      "soundType               69292\n",
      "reviewYN                    0\n",
      "reviewWC                    0\n",
      "genreSorted                 0\n",
      "rcSorted                    0\n",
      "releaseDate                 0\n",
      "releaseYear                 0\n",
      "releaseMonth                0\n",
      "releaseDiff                 0\n",
      "audScoreBins                0\n",
      "runtimeBins                 0\n",
      "boxOfficeBins               0\n",
      "releaseDiffBins             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "merged = select_features(load_csv(\"train\"), load_csv(\"movies\"), row_thresh_null=None)\n",
    "inspect(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More helper functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing predictions on missing reviewtext columns  \n",
    "\n",
    "def predict_on_missing_review_data(pipe, selected_features, merged_train):\n",
    "    missing_reviews_train = merged_train.loc[merged_train['reviewYN'] == 1]\n",
    "    missing_reviews_train.reset_index(drop=True)\n",
    "    X_train_miss_revs = missing_reviews_train.drop('sentiment', axis = 1)\n",
    "    X_train_miss_revs = X_train_miss_revs[selected_features]\n",
    "    y_train_miss_revs = missing_reviews_train['sentiment']\n",
    "    \n",
    "    y_pred_miss_revs = pipe.predict(X_train_miss_revs)\n",
    "#     print('Predictions on rows which had missing reviewText')\n",
    "    print(\"Confusion matrix and f1-score for rows which have no reviewText in X_train: \")\n",
    "    print(confusion_matrix(y_train_miss_revs, y_pred_miss_revs, labels=pipe.classes_))\n",
    "    \n",
    "    return f1_score(y_train_miss_revs, y_pred_miss_revs, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build pipelines for GridSearchCV which goes into the final 'submit' function  \n",
    "def build_pipeline(selected_model=LogisticRegression(), \n",
    "                   selected_features={'txt': ['reviewText']}, \n",
    "                   param_grid=None,\n",
    "                   vocab=None,\n",
    "                   vocab_usage='tfidf',\n",
    "                   strip_accents='unicode', \n",
    "                   add_countvec=False):\n",
    "    # Encoders  \n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    MMscaler = MinMaxScaler()\n",
    "    StdScaler = StandardScaler()\n",
    "    RobScaler = RobustScaler()\n",
    "    \n",
    "    # Text features ['reviewText', 'reviewerName', 'movieid', 'genre', 'director']\n",
    "    tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "    count_vec = CountVectorizer(ngram_range=(1,2))\n",
    "    txt_pipe = Pipeline(steps=[\n",
    "                            (\"tvec\", TfidfVectorizer(ngram_range=(1,3), strip_accents=strip_accents))    # Adjust ngram_range here for reviewText\n",
    "                        ])\n",
    "    txt_pipe_vocab = Pipeline(steps=[\n",
    "                            (\"tvec\", TfidfVectorizer(ngram_range=(1,3), strip_accents=strip_accents, vocabulary=vocab))    # Adjust ngram_range here for reviewText\n",
    "                        ])\n",
    "    txt_pipe_2 = Pipeline(steps=[\n",
    "                            (\"tvec\", TfidfVectorizer(ngram_range=(2,2)))\n",
    "                        ])\n",
    "    txt_pipe_3 = Pipeline(steps=[\n",
    "                            (\"tvec\", TfidfVectorizer(ngram_range=(1,1)))\n",
    "                        ])\n",
    "    txt_pipe_4 = Pipeline(steps=[\n",
    "                            (\"tvec\", TfidfVectorizer(ngram_range=(1,3)))    # Adjust ngram_range here for reviewText\n",
    "                        ])\n",
    "    txt_pipe_title  = Pipeline(steps=[\n",
    "                            (\"tvec\", TfidfVectorizer(ngram_range=(1,5)))    # Adjust ngram_range here for reviewText\n",
    "                        ])\n",
    "    \n",
    "    # Additional pipes for count vectorizer\n",
    "    txt_pipe_countvec = Pipeline(steps=[\n",
    "                            (\"cvec\", CountVectorizer(ngram_range=(1,3), strip_accents=strip_accents))    # Adjust ngram_range here for reviewText\n",
    "                        ])\n",
    "    txt_pipe_countvec_vocab = Pipeline(steps=[\n",
    "                            (\"cvec\", CountVectorizer(ngram_range=(1,3), strip_accents=strip_accents, vocabulary=vocab))    # Adjust ngram_range here for reviewText\n",
    "                        ])\n",
    "    \n",
    "    txt_pipe_countvec_ratingContents = Pipeline(steps=[\n",
    "                            (\"cvec\", CountVectorizer(ngram_range=(1,5)))    # Adjust ngram_range here for reviewText\n",
    "                        ])\n",
    "    txt_pipe_countvec_rcSorted = Pipeline(steps=[\n",
    "                            (\"cvec\", CountVectorizer(ngram_range=(1,1)))    # Adjust ngram_range here for reviewText\n",
    "                        ])\n",
    "    txt_pipe_countvec_genre = Pipeline(steps=[\n",
    "                            (\"cvec\", CountVectorizer(ngram_range=(1,1)))\n",
    "                        ])\n",
    "    txt_pipe_countvec_genreSorted = Pipeline(steps=[\n",
    "                            (\"cvec\", CountVectorizer(ngram_range=(1,5)))\n",
    "                        ])\n",
    "    txt_pipe_countvec_distributor = Pipeline(steps=[\n",
    "                            (\"cvec\", CountVectorizer(ngram_range=(1,3)))    # Adjust ngram_range here for reviewText\n",
    "                        ])\n",
    "    txt_pipe_countvec_title = Pipeline(steps=[\n",
    "                            (\"cvec\", CountVectorizer(ngram_range=(1,5), strip_accents=strip_accents))\n",
    "                        ])\n",
    "    txt_pipe_countvec_movieid = Pipeline(steps=[\n",
    "                            (\"cvec\", CountVectorizer(ngram_range=(1,1)))\n",
    "                        ])\n",
    "    # Basic pipes  \n",
    "    TFs = []\n",
    "    try:\n",
    "        for c in selected_features['cat']:\n",
    "            TFs.append((f\"cat_{c}\", ohe, [c]))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        for n in selected_features['num']:\n",
    "            if n in [\"runtimeMinutes\", \"boxOffice\", \"releaseDiff\"]:\n",
    "                TFs.append((f\"num_{n}\", RobScaler, [n]))\n",
    "            else:\n",
    "                TFs.append((f\"num_{n}\", MMscaler, [n]))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        for t in selected_features['txt']:\n",
    "            if t in ['director', 'reviewerName']:\n",
    "                TFs.append((f\"txt_{t}\", txt_pipe_2, t))\n",
    "            elif t in [\"originalLanguage\", \"releaseYear\"]:\n",
    "                TFs.append((f\"txt_{t}\", txt_pipe_3, t))\n",
    "            elif t in ['genre']:\n",
    "                TFs.append((f\"txt_{t}\", txt_pipe_countvec_genre, t))\n",
    "            elif t in [\"genreSorted\"]:\n",
    "                TFs.append((f\"txt_{t}\", txt_pipe_countvec_genreSorted, t))\n",
    "            elif t in [\"distributor\"]:\n",
    "                TFs.append((f\"txt_{t}\", txt_pipe_countvec_distributor, t))\n",
    "            elif t in ['reviewText']:\n",
    "#                 if vocab and (vocab_usage in [\"tfidf\", \"both\"]):\n",
    "#                     txt_pipe.set_params(tvec__vocabulary=vocab)\n",
    "#                     txt_pipe.set_params(tvec__stop_words='english')\n",
    "                TFs.append((f\"txt_{t}\", txt_pipe, t))\n",
    "            elif t in ['reviewText_2']:\n",
    "                if add_countvec:\n",
    "#                     if vocab and (vocab_usage in [\"count\", \"both\"]):\n",
    "#                         txt_pipe_countvec.set_params(cvec__vocabulary=vocab)\n",
    "#                         txt_pipe_countvec.set_params(cvec__stop_words='english')\n",
    "                    TFs.append((f\"txt_{t}\", txt_pipe_countvec, t))\n",
    "            elif t in ['reviewText_3']:\n",
    "                if vocab_usage == 'tfidf':\n",
    "                    TFs.append((f\"txt_{t}\", txt_pipe_vocab, t))\n",
    "                elif vocab_usage == 'count':\n",
    "                    TFs.append((f\"txt_{t}\", txt_pipe_countvec_vocab, t))\n",
    "            elif t in [\"ratingContents\"]:\n",
    "                TFs.append((f\"txt_{t}\", txt_pipe_countvec_ratingContents, t))\n",
    "            elif t in [\"rcSorted\"]:\n",
    "                TFs.append((f\"txt_{t}\", txt_pipe_countvec_rcSorted, t))\n",
    "            elif t in [\"title\"]:\n",
    "                TFs.append((f\"txt_{t}\", txt_pipe_title, t))\n",
    "            elif t in [\"movieid\"]:\n",
    "                TFs.append((f\"txt_{t}\", txt_pipe_countvec_movieid, t))\n",
    "            else:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Build ColumnTransformer  \n",
    "    ct = ColumnTransformer(transformers=TFs, remainder='drop')\n",
    "\n",
    "    # Build Pipeline\n",
    "    pipe = Pipeline(steps=[('ct', ct), ('model', selected_model)])\n",
    "    print(\"\\nPipeline built successfully.\")\n",
    "\n",
    "    # Use the pipe in GridSearchCV\n",
    "    if param_grid == None:\n",
    "        param_grid_temp = {\"model__C\": [1],\n",
    "                     'model__solver': ['liblinear']}\n",
    "        print(\"Full GridSearchCV pipeline built successfully with basic default param_grid.\\n\")\n",
    "        pipeCV = GridSearchCV(pipe, param_grid_temp, cv=10, scoring=\"f1_micro\", n_jobs=-1)\n",
    "        return pipeCV\n",
    "    \n",
    "    # GridSearchCV if param_grid provided\n",
    "    if param_grid:\n",
    "        pipeCV = GridSearchCV(pipe, param_grid, cv=10, scoring=\"f1_micro\", n_jobs=-1)\n",
    "        print(\"\\nFull GridSearchCV pipeline built successfully.\")\n",
    "        return pipeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_v4(selected_model=LogisticRegression(C=1, solver='liblinear', max_iter=100000), \n",
    "              selected_features={'txt': ['reviewText']}, \n",
    "              param_grid=None,\n",
    "              vocab=None,\n",
    "              vocab_usage='tfidf',\n",
    "              strip_accents='unicode', \n",
    "              add_countvec=False):\n",
    "    \n",
    "    print(\"\\nRunning the submit_v4 function...\")\n",
    "    \n",
    "    # Fine tune selected_features\n",
    "    if add_countvec:\n",
    "        if 'txt' in selected_features.keys():\n",
    "            if \"reviewText_2\" not in selected_features['txt']:\n",
    "                selected_features['txt'].append('reviewText_2')\n",
    "    if vocab:\n",
    "        if 'txt' in selected_features.keys():\n",
    "            if \"reviewText_3\" not in selected_features['txt']:\n",
    "                selected_features['txt'].append('reviewText_3')\n",
    "    print(f\"\\nSelected features: {selected_features}\")\n",
    "\n",
    "    # Build Pipeline\n",
    "    pipe = build_pipeline(selected_model, selected_features, param_grid=param_grid, \n",
    "                          vocab=vocab, vocab_usage='tfidf', \n",
    "                          strip_accents=strip_accents, add_countvec=add_countvec)\n",
    "    print(pipe)\n",
    "\n",
    "    # Features list\n",
    "    features = []\n",
    "    for item in selected_features.values():\n",
    "        features.extend(item)\n",
    "\n",
    "    # Retrain on the whole train.csv file  \n",
    "    merged = select_features(load_csv(\"train\"), load_csv(\"movies\"), row_thresh_null=None)    # Decide if you want to drop any rows containing lot of nulls\n",
    "    if add_countvec:\n",
    "        merged['reviewText_2'] = merged['reviewText']\n",
    "    if vocab:\n",
    "        merged['reviewText_3'] = merged['reviewText']\n",
    "        \n",
    "    X_train = merged.drop(labels=\"sentiment\", axis=1)\n",
    "    y_train = merged[\"sentiment\"]\n",
    "\n",
    "    X_train = X_train[features]\n",
    "    \n",
    "#     # Duplicate reviewText column to use both CountVectorizer and TfidfVectorizer\n",
    "#     if add_countvec and (\"reviewText\" in features):\n",
    "#         X_train['reviewText_2'] = X_train['reviewText']\n",
    "    \n",
    "    # Check1\n",
    "    print(\"\\nCheck 1 complete.\")\n",
    "    print(f\"Shape of X_train: {X_train.shape}\")\n",
    "    print(f\"Features in X_train: {X_train.columns}\")\n",
    "#     print(X_train.head())\n",
    "    \n",
    "#     if not param_grid:\n",
    "#         try:\n",
    "#             print(pipe.named_steps['model'].intercept_, pipe.named_steps['model'].coef_)\n",
    "#         except:\n",
    "#             print(\"Model not trained yet!\")\n",
    "    \n",
    "    # Fit \n",
    "    print(\"\\nTraining started with full pipeline...\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Check2\n",
    "    print(\"\\nCheck 2 complete.\")\n",
    "    print(\"Details of the best model using full pipeline (GridSearchCV) on X_train: \")\n",
    "    print(f\"Best Params: {pipe.best_params_}\")\n",
    "    print(f\"Best Score: {pipe.best_score_}\")\n",
    "    \n",
    "    print(predict_on_missing_review_data(pipe, features, merged))    # Function defined above\n",
    "    \n",
    "        \n",
    "        \n",
    "    # Predict on test.csv file\n",
    "    merged_test = select_features(load_csv(\"test\"), load_csv(\"movies\"))\n",
    "    if add_countvec:\n",
    "        merged_test['reviewText_2'] = merged_test['reviewText']\n",
    "    if vocab:\n",
    "        merged_test['reviewText_3'] = merged_test['reviewText']\n",
    "        \n",
    "    X_test = merged_test.copy()\n",
    "\n",
    "    X_test = X_test[features]\n",
    "    \n",
    "#     # Duplicate reviewText column to use both CountVectorizer and TfidfVectorizer\n",
    "#     if add_countvec and (\"reviewText\" in features):\n",
    "#         X_test['reviewText_2'] = X_test['reviewText']\n",
    "    \n",
    "    # Check3\n",
    "    print(\"\\nCheck 3 complete.\")\n",
    "    print(f\"Shape of X_test: {X_test.shape}\")\n",
    "    print(f\"Features in X_train: {X_test.columns}\")\n",
    "#     print(X_test.head())\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    # Check4\n",
    "    print(\"\\nCheck 4 complete.\")\n",
    "    cv_results_df = pd.DataFrame(pipe.cv_results_)\n",
    "    print(\"Details of the best model using full pipeline (GridSearchCV) on X_train: \")\n",
    "    print(f\"Best Estimator: {pipe.best_estimator_}\")\n",
    "    print(f\"Best Params: {pipe.best_params_}\")\n",
    "    print(f\"Best Score: {pipe.best_score_}\")\n",
    "    print(f\"Best Index: {pipe.best_index_}\")\n",
    "    print(f\"Refit Time: {pipe.refit_time_}\")\n",
    "    print(f\"Shape of CV results dataframe: {cv_results_df.shape}\")\n",
    "    \n",
    "    pred_df = pd.DataFrame(y_pred)\n",
    "    pred_df.columns = [\"sentiment\"]\n",
    "    pred_df.index.name = \"id\"\n",
    "    pred_df.to_csv(\"submission.csv\")\n",
    "    \n",
    "    print(\"\\nSuccessfully created the submission file!!!\")\n",
    "    \n",
    "#     return pipe_cv_results_df\n",
    "    return pipe.cv_results_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
